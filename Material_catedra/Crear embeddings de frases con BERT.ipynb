{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX","timestamp":1660673985709},{"file_id":"1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU","timestamp":1590604015708},{"file_id":"1FsBCkREOaDopLF3PIYUuQxLR8wRfjQY1","timestamp":1559844903389},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1554843110227}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"afa7830d0a10445f9fa3f98951625d9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a745468365ad4444a3d7f8b4141cbd74","IPY_MODEL_ce76772a489441368c074051a55f787f","IPY_MODEL_46fb965532034bbdb2516206851547e4"],"layout":"IPY_MODEL_9028991439bb453e9d56ce064c2d3cbc"}},"a745468365ad4444a3d7f8b4141cbd74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3500f886a663454391131dc535e80803","placeholder":"​","style":"IPY_MODEL_45357a1c03e742d897ec15e12bf749db","value":"Downloading vocab.txt: 100%"}},"ce76772a489441368c074051a55f787f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f5be36d9b3c4fbb9684b7d833ac3bcd","max":247723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e057eff2b944498a99d2fb3aeebc6f54","value":247723}},"46fb965532034bbdb2516206851547e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_023bf842a1f74dbcb3f81f491537ae94","placeholder":"​","style":"IPY_MODEL_4d1ff0254fe940918535a4e2b1317d6d","value":" 242k/242k [00:00&lt;00:00, 350kB/s]"}},"9028991439bb453e9d56ce064c2d3cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3500f886a663454391131dc535e80803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45357a1c03e742d897ec15e12bf749db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f5be36d9b3c4fbb9684b7d833ac3bcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e057eff2b944498a99d2fb3aeebc6f54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"023bf842a1f74dbcb3f81f491537ae94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d1ff0254fe940918535a4e2b1317d6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d7c5012765b4a26a057b0dc19ec6467":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_274b83c6f9f64827b87d2a9542131361","IPY_MODEL_853fc4e8ab7941edaef766882934a515","IPY_MODEL_e026be0183bd4f73b85db99ec5c85766"],"layout":"IPY_MODEL_9515f0bd4bb245c49ba5e7e64bc98d66"}},"274b83c6f9f64827b87d2a9542131361":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b29c87784e044ce68810f3f14ff47111","placeholder":"​","style":"IPY_MODEL_e72df24b927543f8b63ddd72252269a2","value":"Downloading special_tokens_map.json: 100%"}},"853fc4e8ab7941edaef766882934a515":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85866533fb974e628789ea39134444ac","max":134,"min":0,"orientation":"horizontal","style":"IPY_MODEL_073eb60742d04f14adac167da24b8fc8","value":134}},"e026be0183bd4f73b85db99ec5c85766":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_292ad9a4ad4741fd93e4d5827244b5cb","placeholder":"​","style":"IPY_MODEL_3aa83c1c087d4c8888e68cc90c8ebca1","value":" 134/134 [00:00&lt;00:00, 2.17kB/s]"}},"9515f0bd4bb245c49ba5e7e64bc98d66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b29c87784e044ce68810f3f14ff47111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72df24b927543f8b63ddd72252269a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85866533fb974e628789ea39134444ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"073eb60742d04f14adac167da24b8fc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"292ad9a4ad4741fd93e4d5827244b5cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aa83c1c087d4c8888e68cc90c8ebca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b36af25b94a14992a1425e403c42fe6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3ee4d6897184e52a216041e789a05f3","IPY_MODEL_5286ba2c63ae43dd9933f2509f025366","IPY_MODEL_3a89b933727f4ab38c5f1f448b8d75a6"],"layout":"IPY_MODEL_e997b6f84c9f43aabad59f9cfbf104d9"}},"c3ee4d6897184e52a216041e789a05f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a82bfba4491f4c85a0bcd16fcfdfe2a2","placeholder":"​","style":"IPY_MODEL_cae9345c9d4149f8a99b1d2bef831c68","value":"Downloading tokenizer_config.json: 100%"}},"5286ba2c63ae43dd9933f2509f025366":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b64f69f9c34121950ac6ffbcfbf303","max":310,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8548ce2c8559422b847d30c2b0c0304c","value":310}},"3a89b933727f4ab38c5f1f448b8d75a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a467a5eb57144d47aaab896f2345a8e5","placeholder":"​","style":"IPY_MODEL_816749f765eb46bab6b4fbc79a1aab9d","value":" 310/310 [00:00&lt;00:00, 6.08kB/s]"}},"e997b6f84c9f43aabad59f9cfbf104d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a82bfba4491f4c85a0bcd16fcfdfe2a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cae9345c9d4149f8a99b1d2bef831c68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5b64f69f9c34121950ac6ffbcfbf303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8548ce2c8559422b847d30c2b0c0304c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a467a5eb57144d47aaab896f2345a8e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"816749f765eb46bab6b4fbc79a1aab9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0085a7af5d604536be0f5618ebba5dcb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d11941e25724635aa6d8429e128ee25","IPY_MODEL_1a22f7b8c5da449b8bc295698fdbee0d","IPY_MODEL_dd31d27a2c67419f9f638f1803e286b3"],"layout":"IPY_MODEL_5d33405fe0974274a4554bb0d74b7d03"}},"3d11941e25724635aa6d8429e128ee25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9548049ac6a9459f9ea6a6438f18c708","placeholder":"​","style":"IPY_MODEL_907663e3f64f446a8bfc71182fc7fa53","value":"Downloading config.json: 100%"}},"1a22f7b8c5da449b8bc295698fdbee0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e53cb9e5eb24b479d23dbed4c57740d","max":650,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1bca173bfdf4461bf7e45f8e3a24224","value":650}},"dd31d27a2c67419f9f638f1803e286b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d04d77b2154a21a07155cfd69353a4","placeholder":"​","style":"IPY_MODEL_8dedd76fae3445928aa461cfd9e28a96","value":" 650/650 [00:00&lt;00:00, 16.1kB/s]"}},"5d33405fe0974274a4554bb0d74b7d03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9548049ac6a9459f9ea6a6438f18c708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"907663e3f64f446a8bfc71182fc7fa53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e53cb9e5eb24b479d23dbed4c57740d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1bca173bfdf4461bf7e45f8e3a24224":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86d04d77b2154a21a07155cfd69353a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dedd76fae3445928aa461cfd9e28a96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d933aae4eb34c9a9c7c27fd5ddbe311":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_556b85e4965b4829a89266f6388fd78e","IPY_MODEL_1675c6a07b6d474487c223280e9ede53","IPY_MODEL_6110fb15769e4eb8a8d4ebf515f0c393"],"layout":"IPY_MODEL_84f00c81539b4c068cb8824262164038"}},"556b85e4965b4829a89266f6388fd78e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2042066d75ac4c98b720cfd5443a2528","placeholder":"​","style":"IPY_MODEL_856124634e4c445ebe7a6afa1698ad13","value":"Downloading pytorch_model.bin: 100%"}},"1675c6a07b6d474487c223280e9ede53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_641bd29a126749deaeaf01e6ae96af98","max":439621341,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23494f7596da458cb15ab8e13f2416b9","value":439621341}},"6110fb15769e4eb8a8d4ebf515f0c393":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a09534b8e7e648b28f86adb3948041c8","placeholder":"​","style":"IPY_MODEL_46025a14d20b4d839a7cbbf7c7be1de9","value":" 419M/419M [00:22&lt;00:00, 21.8MB/s]"}},"84f00c81539b4c068cb8824262164038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2042066d75ac4c98b720cfd5443a2528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"856124634e4c445ebe7a6afa1698ad13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"641bd29a126749deaeaf01e6ae96af98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23494f7596da458cb15ab8e13f2416b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a09534b8e7e648b28f86adb3948041c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46025a14d20b4d839a7cbbf7c7be1de9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["# Embeddings con BERT\n","\n","\n","In este notebook, analizaremos en profundidad las embeddings de palabras producidos por BERT, y veremos cómo comenzar a utilizar BERT produciendo embeddings de frases a partir de un modelo ya entrenado.\n","\n","Esta notebook está derivada en un 95% derived de blog  [\"BERT Word Embeddings Tutorial\"](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)."]},{"cell_type":"markdown","metadata":{"id":"dYapTjoYa0kO"},"source":["# Introducción\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WoitNQMWA1bt"},"source":["\n","### Qué es BERT?\n","\n","BERT (Bidirectional Encoder Representations from Transformers), lanzado a finales de 2018, es el modelo que utilizaremos en este tutorial. Es un modelo que aplica mucha de las técnicas que se usan en modelos más complejos más nuevos.\n","\n","Se puede usar BERT modelos para extraer features a partir de texto, o ajustarlos para una tarea específica (clasificación, reconocimiento de entidades, respuesta a preguntas, etc.) con sus propios datos para generar predicciones.\n"]},{"cell_type":"markdown","metadata":{"id":"q-dDVmXAA3At"},"source":["### Por qué usar embeddings de BERT?\n","\n","Vamos a utilizar a BERT para genera features, concretamente vectores de embeddings de palabras y frases.\n","\n"," ¿Qué podemos hacer con estos vectores?\n","\n"," Estos vectores se utilizan como features de alta calidad para los para entrenar modelos. La manera tradicional en un un modelo BOW es que las palabras se representaban como indices (one-hot encoding) o, de forma más útil, como embeddings únicos para cada palabras como los generados por Word2Vec o Fasttext. BERT ofrece una ventaja sobre modelos como Word2Vec: mientras que cada palabra tiene una representación fija en Word2Vec, independientemente del contexto en el que aparezca, BERT produce representaciones de palabras que se basan en las palabras que las rodean.\n","\n","Por ejemplo, dadas dos las frases:\n","\n","\"*The man was accused of robbing a bank.*\"  y\n","\n","\"*The man went fishing by the bank of the river.*\"\n","\n","Word2Vec produciría los mismos embeddings para las 2 ocurrencias de \"*bank*\", mientras que con BERT la incrustación de palabras para \"*bank*\" es diferente en cada oración. Además de capturar diferencias obvias como la polisemia, los embeddings de palabras sensibles al contexto capturan otros tipos de información que resultan en representaciones de características más precisas, lo que a su vez mejora el rendimiento del modelo.\n"]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["# 1. Cargando un modelo de BERT  Pre-entrenado"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Aqui instalamos la versión de BERT en pytorch provista por Hugging Face.\n","\n","En Google Colab, deberá volver a instalar esta bibliotecas cada ver que se reconecta."]},{"cell_type":"code","metadata":{"id":"1RfUN_KolV-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662071429436,"user_tz":180,"elapsed":14200,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"d92aa865-a335-45ac-8bfb-8b4f17e2c93a"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 36.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 67.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"yF9g_Q8cYax_"}},{"cell_type":"markdown","metadata":{"id":"JSXImOxMPdNg"},"source":["Carguemos el modelo BERT pre-entrenado, el tokenizador de BERT, y pytorch para hacer andar todo.\n","Vamos a utilizar un [modelo ya entrenado para español](https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner), sensible a mayúsculas y minúsculas, que es a su vez un modelo destilado de [BETO](https://github.com/dccuchile/beto).\n","\n","\n","La biblioteca `transformers` de HuggingFace provee varias clases útiles para aplicar BERT a diferentes tareas (token classification, text classification, ...). Aquí usamos un modelo `BertModel` que no fue entrenado para ninguna tarea específica, y es una buena opción para usar BERT solo para extraer embeddings."]},{"cell_type":"code","metadata":{"id":"lJEnBJ3gHTsQ","executionInfo":{"status":"ok","timestamp":1662075947441,"user_tz":180,"elapsed":7764,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"62969b44-3628-4005-c231-1f3f387913e8","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["afa7830d0a10445f9fa3f98951625d9b","a745468365ad4444a3d7f8b4141cbd74","ce76772a489441368c074051a55f787f","46fb965532034bbdb2516206851547e4","9028991439bb453e9d56ce064c2d3cbc","3500f886a663454391131dc535e80803","45357a1c03e742d897ec15e12bf749db","7f5be36d9b3c4fbb9684b7d833ac3bcd","e057eff2b944498a99d2fb3aeebc6f54","023bf842a1f74dbcb3f81f491537ae94","4d1ff0254fe940918535a4e2b1317d6d","7d7c5012765b4a26a057b0dc19ec6467","274b83c6f9f64827b87d2a9542131361","853fc4e8ab7941edaef766882934a515","e026be0183bd4f73b85db99ec5c85766","9515f0bd4bb245c49ba5e7e64bc98d66","b29c87784e044ce68810f3f14ff47111","e72df24b927543f8b63ddd72252269a2","85866533fb974e628789ea39134444ac","073eb60742d04f14adac167da24b8fc8","292ad9a4ad4741fd93e4d5827244b5cb","3aa83c1c087d4c8888e68cc90c8ebca1","b36af25b94a14992a1425e403c42fe6d","c3ee4d6897184e52a216041e789a05f3","5286ba2c63ae43dd9933f2509f025366","3a89b933727f4ab38c5f1f448b8d75a6","e997b6f84c9f43aabad59f9cfbf104d9","a82bfba4491f4c85a0bcd16fcfdfe2a2","cae9345c9d4149f8a99b1d2bef831c68","c5b64f69f9c34121950ac6ffbcfbf303","8548ce2c8559422b847d30c2b0c0304c","a467a5eb57144d47aaab896f2345a8e5","816749f765eb46bab6b4fbc79a1aab9d","0085a7af5d604536be0f5618ebba5dcb","3d11941e25724635aa6d8429e128ee25","1a22f7b8c5da449b8bc295698fdbee0d","dd31d27a2c67419f9f638f1803e286b3","5d33405fe0974274a4554bb0d74b7d03","9548049ac6a9459f9ea6a6438f18c708","907663e3f64f446a8bfc71182fc7fa53","7e53cb9e5eb24b479d23dbed4c57740d","e1bca173bfdf4461bf7e45f8e3a24224","86d04d77b2154a21a07155cfd69353a4","8dedd76fae3445928aa461cfd9e28a96"]}},"source":["import torch\n","from transformers import BertTokenizer, BertModel\n","\n","import logging\n","#logging.basicConfig(level=logging.INFO)\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Nombre del modelo en  Huggingface\n","BERT_MODEL = 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n","# Algunos modelos BERT disponibles en español:\n","#    'mrm8488/bert-spanish-cased-finetuned-ner'\n","#    'dccuchile/bert-base-spanish-wwm-uncased'\n","#\n","# Cargar el mismo tokenizador usado para entrenar el modelo\n","tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa7830d0a10445f9fa3f98951625d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7c5012765b4a26a057b0dc19ec6467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer_config.json:   0%|          | 0.00/310 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36af25b94a14992a1425e403c42fe6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0085a7af5d604536be0f5618ebba5dcb"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Tlv3VlPnKKHN"},"source":["# 2. Formato de la Entrada a BERT\n","\n","El modelo pre-entrenado de BERT espera un formato específico del texto de entrada. Debe contener:\n","\n","1. Un **token especial, `[SEP]`,** para marcar el fin de una frase, o la separacion entre 2 frases.\n","2. Un **token expecial, `[CLS]`,** al comienzo (sí, al comienzo) del texto. Este token se usa para entrenar modelos de BERT para clasificación, pero BERT espera este token sin importar para qué es el modelo.\n","3. Tokens que coinciden con el vocabulario fijo con el que fue entrenadoBERT\n","4. Los **Token IDs** derivados de los tokens, generado por el tokenizador de  BERT\n","5. **Mask IDs** (también llamadas \"attention masks\") que se usan para indicar que parte de la frase de entrada (BERT usa tamaño fijo) son  tokens (representados por un 1) and cuales hay que ignorar  (representeados por un 0).\n","6. **Segment IDs** (También llamados \"token type ids\") que son usados para distiguir las 2 frases cuando se usan 2 frases consecutivas *A* y *B*: 0 significa la frase *A* y 1 significa la frase  *B*. Para los modelos de clasificación hay 1 sola frase *A*, para los de completar la frase hay *A* y *B* .  Hay un  token type id por cada token.\n","7. **Positional Embeddings** que se usan para indicar la posición del token dentro de la frase.\n","\n","Por suerte la biblioteca  `transformers` se encarga de la mayoría de estos problemas ald usar la función  `tokenizer.encode_plus`.\n","\n","Para un ejemplo de como usar `tokenizer.encode_plus`, vea la notebook de fine tuning."]},{"cell_type":"markdown","metadata":{"id":"diVtyCJCurxJ"},"source":["## 2.1. Tokens Epeciales\n","La entrada a BERT pueden ser una o 2 frases consecutivas, y se usa el token especial `[SEP]` para separarlas. El token especial `[CLS]` siempre debe aparecer al comienzo del texto, aun cuando no se use.\n","\n","Tanto `[SEP]` como `[CLS]` son  *obligatorios siempre*, however, aún si tenemos 1 sola frase y no estamos usando a BERT para clasificar, debido a que así fue entrenado el modelo.\n","\n","**Ejemplo de 1 sola frase de Entrada**:\n","\n","`[CLS] The man went to the store. [SEP]`\n","\n","**Ejemplo de 2 frases de Entrada consecutivas**:\n","\n","`[CLS] The man went to the store. [SEP] He bought a gallon of milk.`\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gsyrAwYvBfC"},"source":["## 2.2. Tokenization"]},{"cell_type":"markdown","metadata":{"id":"2WafgQPLAWmo"},"source":["BERT tiene su propio tokenizador, y necesitamos procesar cualquier frase con el mismo tokenizador usado al entrenar.\n","\n","**IMPORTANTE:** BERT está entrenado con una ventana de 512 tokens, asi que usará hasta los primeros 512 tokens de la frase y descartará el resto."]},{"cell_type":"code","metadata":{"id":"Pg0P9rFxJwwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662079986911,"user_tz":180,"elapsed":404,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"06ab7193-20fe-44e5-820e-4d454a00826b"},"source":["text = \"Quiero conseguir los embeddings de esta frase.\"\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","# Tokenizar la frase con el mismo tokenizador usado para entrenar al modelo BERT.\n","tokenized_text = tokenizer.tokenize(marked_text)\n","\n","print (tokenized_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'quiero', 'conseguir', 'los', 'emb', '##ed', '##ding', '##s', 'de', 'esta', 'frase', '.', '[SEP]']\n"]}]},{"cell_type":"markdown","source":["Como se transforma una palabra en tokens? Veamos como se tokeniza la palabra  \"*embeddings*\":"],"metadata":{"id":"WOXd14gdSDdB"}},{"cell_type":"code","source":["tokenizer.tokenize(\"embeddings\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCt3Imj6Sioc","executionInfo":{"status":"ok","timestamp":1662079951874,"user_tz":180,"elapsed":442,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"3186a33e-7881-432c-aad9-4b7578cd7f95"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['emb', '##ed', '##ding', '##s']"]},"metadata":{},"execution_count":147}]},{"cell_type":"markdown","metadata":{"id":"Q51eN4KAkbIJ"},"source":["Como se ve la transformación de palabra a tokens no es 1:1; la palabra se dividió en varios n-gramas. Los 2 hashes (##)  que preceden el comienzo de la palabra son la manera de denotar que ese n-grama **no** es prefijo de la palabra que lo contiene. De esta manera, el n-grama '##bed' es diferente del n-grama para \"bed\"; el primero se usa cuando el n-grama 'bed' occure adentro de una palabra más larga, y el 2do para representar la palabra \"bed\".\n","\n","Internamente, BERT usa un tokenizer llamado *WordPiece*. Este tokenizer crea n-gramas hasta un vocabulario de tamaño máximo, y conserva los n-gramas más utiles para representar el texto. BERT típicamente tiene un tamaño máximo de tokens de 30.000. Este vocabulario contiene:\n","\n","1. palabras completas\n","2. n-gramas que aparecen o solos o al comienzo de una palabra,\n","3. n-gramas que no son parte del comienzo de una palabra, que van precedidos de '##',\n","4. Caracteres sueltos\n","\n","El resultado es que al ver una encontrar una palabra desconocida, en vez de asignarle a todas las palabras desconocidas el mismo tokene 'OOV' or 'UNK,' la palabra aún puede ser parcialmente representada por los n-gramas que sí apareciero en la colencción de entrada.\n","\n","Entonces, en vez de asignarle un embedding a cada palabra, BERT el asigna un embedding a cada n-grama de la palabra.\n","\n","(Para ver más información sobre WordPiece, vea el  [paper original ](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) y [Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf).)\n","\n","La llamada a `from_pretrained` descargará el modelo automáticamente.\n"]},{"cell_type":"code","metadata":{"id":"Mq2PKplWfbFv","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5d933aae4eb34c9a9c7c27fd5ddbe311","556b85e4965b4829a89266f6388fd78e","1675c6a07b6d474487c223280e9ede53","6110fb15769e4eb8a8d4ebf515f0c393","84f00c81539b4c068cb8824262164038","2042066d75ac4c98b720cfd5443a2528","856124634e4c445ebe7a6afa1698ad13","641bd29a126749deaeaf01e6ae96af98","23494f7596da458cb15ab8e13f2416b9","a09534b8e7e648b28f86adb3948041c8","46025a14d20b4d839a7cbbf7c7be1de9"]},"executionInfo":{"status":"ok","timestamp":1662075992298,"user_tz":180,"elapsed":27271,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"aa8c7eaf-191a-43c9-e056-fb40b26107c8"},"source":["# Cargar los pesos del modelo pre-entrenado\n","model = BertModel.from_pretrained(BERT_MODEL,\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n","                                  )\n","\n","# Setear al modelo en modo  \"evaluacion\", que significa que no va a hacer entrenamiento (backpropagation), solo usar al modelo.\n","model.eval()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/419M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d933aae4eb34c9a9c7c27fd5ddbe311"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(31002, 768, padding_idx=1)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["## Ejemplo de algunos tokens (n-gramas):\n","\n","Estos son algunos ejemplos de tokens en el vocabulario del modelo. Recuerde que los tokens con ## son n-gramas o caracteres individuales."],"metadata":{"id":"rQC2YxFTQYnd"}},{"cell_type":"code","metadata":{"id":"1z1SzuTrqx-7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662076048389,"user_tz":180,"elapsed":700,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"5273d993-daa8-4fa0-a288-7dff105035c1"},"source":["list(tokenizer.vocab.keys())[5000:5020]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['terminar',\n"," '##ht',\n"," 'escal',\n"," '##izado',\n"," 'lab',\n"," 'ofer',\n"," 'terrorismo',\n"," 'cuentas',\n"," 'accidente',\n"," 'austral',\n"," '##sen',\n"," 'deberían',\n"," 'volvió',\n"," 'imposible',\n"," '##idor',\n"," 'miles',\n"," 'visita',\n"," 'brasil',\n"," 'fru',\n"," '55']"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"HoF3LC47VgBb"},"source":["Después de dividir la frase en token, aún tenemos que convertir a la frase en una lista de índices de cada token.\n","\n","\n","\n","## Cómo codificar una frase en índices de  token:"]},{"cell_type":"code","metadata":{"id":"XYjcYJuXoAQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081086192,"user_tz":180,"elapsed":437,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"fcc49a63-bf72-4f2c-c4fe-fa0fb54c579e"},"source":["# Defino un ejemplo con distintos significados de la palabra \"cura\" y \"caras\"\n","text = \"la enfermedad del cura es demasiado cara, no tiene tratamiento conocido, dijo poniendo una cara triste.\"\n","\n","# Agregar los tokens especiales.\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","# Tokenizar la frase.\n","tokenized_text = tokenizer.tokenize(marked_text)\n","\n","# Mapear los tokens a indices.\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","# Imprimir cada token con su indice.\n","for tup in zip(tokenized_text, indexed_tokens):\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]             4\n","la            1,032\n","enfermedad    4,968\n","del           1,081\n","cura          7,933\n","es            1,028\n","demasiado     2,668\n","cara          2,618\n",",             1,019\n","no            1,054\n","tiene         1,394\n","tratamiento   4,440\n","conocido      3,638\n",",             1,019\n","dijo          1,921\n","poniendo      8,533\n","una           1,091\n","cara          2,618\n","triste        5,542\n",".             1,008\n","[SEP]             5\n"]}]},{"cell_type":"markdown","metadata":{"id":"if6C_iCULU60"},"source":["## 2.3. Attention masks\n","\n","Las attention mask solo se usan para clasificar, e indican a que tokens hay que prestarle atención: 1 a loss  que NO estan enmascarados (prestales atención), 0 a los que estan enmascarados (ignorarlos).\n","\n","Dado que vamos a usar BERT para generar embeddings, le vamos a poner 1 a todos los tokens."]},{"cell_type":"code","metadata":{"id":"u_jEkVKxJMc0"},"source":["# Marcar cada token como \"prestarle atención\".\n","attention_mask = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-nY9LASLr2L"},"source":["# 3. Extrayendo Embeddings\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UeQNEFbUgMSf"},"source":["## 3.1. Entendiendo al modelo\n"]},{"cell_type":"markdown","metadata":{"id":"HKTlTS_sfuAe"},"source":["BERT es una DNN con 12 capas escondidas (hidden layers). Podemos ver a todas las Las capas escondidas, que está en el código abajo están en la variable `hidden_states`. El contenido de `hidden_states` es un objecto de 4 dimensiones, en el siguiente orden:\n","\n","1. El número de capa (layer) en la red (13 layers)\n","2. El número de  batch  (1 sentence, solo usado al entrenar)\n","3. El indice de palabra o token (21 tokens in our sentence)\n","4. La dimension del vector de cada token en la capa escondida (768). No confundir con la cantidad máxima de tokens que acepta BERT, que es 512.\n","\n","Pero ¿por qué hay 13 layers si BERT es un modelo de 12 hidden layers? Respuesta: Hay 13 porque La entrada a BERT son embeddings de tokens, no tokens. La 1ra capa son los embeddings de entrada.\n","\n","Esto significa que hay 179172 (13 x 18 x 768) valores values en la red para representar la frase.\n","\n","\n","Veamos un poco que forma tienen las capas escondidas de BERT:\n","\n","(NOTA al margen: la llamada a  `torch.no_grad` le dice a PyTorch que no construya el grafo para computar derivadas. Dado que aquí no vamos a usar backpropagation (ya está entrenado) esto ahorra memoria.)"]},{"cell_type":"code","metadata":{"id":"nN0QTZwiMzeq"},"source":["# Convertir indices a vectores de PyTorch\n","tokens_tensor = torch.tensor([indexed_tokens])\n","attention_mask_tensor = torch.tensor([attention_mask])\n","\n","# Hacer pasar al texto por  BERT, y juntar las capas escondidas\n","with torch.no_grad():\n","\n","    outputs = model(tokens_tensor, attention_mask_tensor)\n","\n","    # Evaluating the model will return a different number of objects based on\n","    # how it's  configured in the `from_pretrained` call earlier. In this case,\n","    # becase we set `output_hidden_states = True`, the third item will be the\n","    # hidden states from all layers. See the documentation for more details:\n","    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eI_uxiW7eRWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081100553,"user_tz":180,"elapsed":5,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"d07bc062-0c9f-4216-d0a5-24cf5078280c"},"source":["print (\"Cantidad de layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n","layer_i = 0\n","\n","print (\"Cantidad de batches:\", len(hidden_states[layer_i]))\n","batch_i = 0\n","\n","print (\"Cantidad de tokens:\", len(hidden_states[layer_i][batch_i]))\n","token_i = 0\n","\n","print (\"Cantidad de hidden units en cada capa (dimension de embedding):\", len(hidden_states[layer_i][batch_i][token_i]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of layers: 13   (initial embeddings + 12 BERT layers)\n","Number of batches: 1\n","Number of tokens: 21\n","Number of hidden units: 768\n"]}]},{"cell_type":"code","metadata":{"id":"0CcY_oRwcHlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081104094,"user_tz":180,"elapsed":5,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"9b1f5660-b7b7-4f13-c38f-eacb3d7222f4"},"source":["# `hidden_states` es una lista de  Python.\n","print('Tip de los valores de cada capa escondida : ', type(hidden_states))\n","\n","# Cada capa en la lista es un tensor.\n","print('Forma del vector para cada capa: ', hidden_states[0].size())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of hidden_states:  <class 'tuple'>\n","Tensor shape for each layer:  torch.Size([1, 21, 768])\n"]}]},{"cell_type":"code","metadata":{"id":"pTJV8AFFcLbL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081106209,"user_tz":180,"elapsed":427,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"4c7002df-c041-460a-a33c-2d45ad568b24"},"source":["# Concatenar los vectores de todas las capas.\n","token_embeddings = torch.stack(hidden_states, dim=0)\n","\n","# El tamaño del embedding va a ser = [13 x 1 x 21 x 768]. Estos numeros salen de:\n","# 13 = number of capas en la DNN = 1 embedding + 12 encoding layers\n","# 1 = 1 frase a codificar (\"batches\", en la jerga de BERT)\n","# 21 = cantidad de tokens en la frase; si cambia la frase cambia este número\n","# 768 = dimensiones de cada embedding de 1 token\n","token_embeddings.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 1, 21, 768])"]},"metadata":{},"execution_count":212}]},{"cell_type":"markdown","metadata":{"id":"rnBv2TUNhzf4"},"source":["Descartemos la dimension  \"batches\" porque no la necesitamos:."]},{"cell_type":"code","metadata":{"id":"En4JZ41fh6CI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081107899,"user_tz":180,"elapsed":4,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"089cafc4-9095-4f2c-d4cb-925bd4cf5bd8"},"source":["# Borra la dimension 1 = \"batches\".\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","\n","token_embeddings.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 21, 768])"]},"metadata":{},"execution_count":213}]},{"cell_type":"markdown","metadata":{"id":"YVzRfvkbe-Yp"},"source":["Y finalmente, ordenamos las dimensiones como [cantidad de  tokens, cantidad de capas, dimension de embedding en cada capa]:"]},{"cell_type":"code","metadata":{"id":"AtDVE58cdeYp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081109469,"user_tz":180,"elapsed":4,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"6876e857-dbfa-45c0-ba5b-590e54c002e0"},"source":["# Swap dimensions 0 and 1.\n","token_embeddings = token_embeddings.permute(1,0,2)\n","\n","token_embeddings.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([21, 13, 768])"]},"metadata":{},"execution_count":214}]},{"cell_type":"markdown","metadata":{"id":"Ey5RhOQ7NGtz"},"source":["## 3.3. Creaando embeddings de palabras y frases\n","\n","BERT no tiene una \"capa que retorna el embedding\". Lo que tenemos es las capas escondidas de BERT. Queremos conseguir un vector para cada palabra y para cada frase, pero en cambio para cada token tenemos 13 vectores diferentes (1 por cada), cada uno de largo 768 (la dimension de los emdeddings).\n","\n","Pero ¿Y que hacemos con esas capas?\n","\n","Para conseguir embeddings que podamos usar, de alguna manera necesitamos o elegir una capa, o combinar capas.\n","\n","¿Qué combinación de capas es mejor?\n","\n","Desafortunadamente la respuesta es \"depende\", pero hay algunas respuestas que funcionan la mayoría de las veces.\n"]},{"cell_type":"markdown","metadata":{"id":"76TdtFH8NM9q"},"source":["### Generando Vectores para Tokens\n","\n","Vamos a explorar 2 maneras de crear estos vectores.\n","\n","\n","#### Método A para generar embeddings de tokens:  Concatenación\n","\n"," **concatenamos** los vectores re las 4 últimas capas. Como cada capa tiene dimensión 768, por cada palabra nos da un vector de largo  `4 x 768 = 3,072`."]},{"cell_type":"code","metadata":{"id":"pv42h9jANMRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081111696,"user_tz":180,"elapsed":3,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"1de88eb4-7d21-4ad5-9d60-56bccd018b29"},"source":["# Stores the token vectors, with shape [nwords x 3,072]\n","token_vecs_cat = []\n","\n","# `token_embeddings` is a [nsentences x nwords x 768] tensor.\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","\n","    # `token` is a [12 x 768] tensor\n","\n","    # Concatenate the vectors (that is, append them together) from the last\n","    # four layers.\n","    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n","\n","    # Use `cat_vec` to represent `token`.\n","    token_vecs_cat.append(cat_vec)\n","\n","print ('Shape is: %d tokens x %d dimensions for each token' % (len(token_vecs_cat), len(token_vecs_cat[0])))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape is: 21 tokens x 3072 dimensions for each token\n"]}]},{"cell_type":"markdown","metadata":{"id":"VnWaByfelM-e"},"source":["#### Método B para para generar embeddings de tokens: Sumar\n","\n","Otro método es, en vez de concatenar, sumar las 4 últimas capas, dado que todas las capas generan un vector de dimensión 768 como salida."]},{"cell_type":"code","metadata":{"id":"j4DKDtFwiF0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081112875,"user_tz":180,"elapsed":6,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"9e64d314-7b1f-4bbb-d9cd-fe180086fd07"},"source":["# Stores the token vectors, with shape [nwords x 768]\n","token_vecs_sum = []\n","\n","# `token_embeddings` is a [nwords x nlayers x 768] tensor.\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","\n","    # `token` is a [nwords x 768] tensor\n","\n","    # The vector of a word will be the sum of the vectors from the last four layers.\n","    sum_vec = torch.sum(token[-4:], dim=0)\n","\n","    # Use `sum_vec` to represent `token`.\n","    token_vecs_sum.append(sum_vec)\n","\n","print ('Shape is: %d tokens x %d dimensions for each token' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape is: 21 tokens x 768 dimensions for each token\n"]}]},{"cell_type":"markdown","source":["Sin importar como transformemos un token a un embedding (concatenando o sumando), aún necesitamos combinar los embeddings de todos los tokens en 1 solo vector."],"metadata":{"id":"zvYro422sTa1"}},{"cell_type":"markdown","metadata":{"id":"mQaco6jRLkXn"},"source":["### Creando Embeddings para la frase: Promediando\n"]},{"cell_type":"markdown","metadata":{"id":"uuul6iQqnXT2"},"source":["Una manera simple y totalmente empírica de de combinar los vectores de tokens en 1 vector de frase es promediarlos, es promediar los vectores de cada token generados por la anteúltima capa. Esto nos da una representación de la frase de dimensión 768."]},{"cell_type":"code","metadata":{"id":"Zn0n2S-FWZih"},"source":["# `hidden_states` is a list of tensors of size [nwords x 768], len(hidden_states) = 13 = layers\n","\n","# `token_vecs` is a tensor with shape [nwords x 768]\n","token_vecs = hidden_states[-2][0]\n","# Calculate the average of all token vectors in the sentence.\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQv0FL8VWadn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662080995072,"user_tz":180,"elapsed":8,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"a494a789-a28c-4770-a0d3-72736f1ce1e8"},"source":["print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our final sentence embedding vector of shape: torch.Size([768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"TqYcrAipfE3E"},"source":["## 3.4. Confirmando que los vectores dependen del contexto\n","\n","Para confirmar que los vectores dependen del contexto,veamos los embeddings generados por  2 ocurrencias de las palabras \"cara\" y \"cura\" en la siguiente frase:\n","\n","\"\"la **cura** de la enfermedad del **cura** es demasiado **cara**, dijo con **cara** triste.\"\n","\n","Veamos los indices de las ocurrencias de las palabras \"cara\"y \"cura\" en la frase de ejemplo.\n","\n","## Imprimir los tokens de la frase junto con sus indices:"]},{"cell_type":"code","metadata":{"id":"DNiRsEh9cmWz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081119934,"user_tz":180,"elapsed":4,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"64cc0f12-782c-453a-e2f0-fa244435b7be"},"source":["for i, token_str in enumerate(tokenized_text):\n","  print (i, token_str)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 [CLS]\n","1 la\n","2 enfermedad\n","3 del\n","4 cura\n","5 es\n","6 demasiado\n","7 cara\n","8 ,\n","9 no\n","10 tiene\n","11 tratamiento\n","12 conocido\n","13 ,\n","14 dijo\n","15 poniendo\n","16 una\n","17 cara\n","18 triste\n","19 .\n","20 [SEP]\n"]}]},{"cell_type":"markdown","metadata":{"id":"AEhBIA5RlS8-"},"source":["Como se ve, la palabra \"cara\" en la frase tiene 2 tokens con indices 2 y 17 .\n","\n","Ahora podemos tratar de imprimir los vectores para compararlos. Para este análisis vamos a usar el metodo en que sumamos las 4 últimas capas (método B) y vamos a imprimir las primeras  10 dimensiones de cada vector.\n"]},{"cell_type":"code","metadata":{"id":"tBa6vRHknSkv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662081127154,"user_tz":180,"elapsed":473,"user":{"displayName":"Fernando Das Neves","userId":"06324247755041763216"}},"outputId":"c373d8e8-a172-43cb-e946-cf18fbf2fa83"},"source":["print('Primeras 10 dimensiones de los vectores correspondientes a \"cara\".')\n","print('')\n","print('cara (en cdemasiado cara) ', str(token_vecs_sum[7][:10]))\n","print('cara (en \"cara triste\")   ', str(token_vecs_sum[17][:10]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras 10 dimensiones de los vectores correspondientes a \"cara\".\n","\n","cara (en cdemasiado cara)  tensor([ 3.5607,  1.4791,  3.2746, -0.8986,  7.8802,  1.3034,  1.6476, -0.6275,\n","        -2.8693, -3.3488])\n","cara (en \"cara triste\")    tensor([-1.7550, -3.7085, -1.6301,  2.0678,  6.7078, -1.3986, -4.5931, -2.3785,\n","        -0.7294, -0.7507])\n"]}]},{"cell_type":"markdown","metadata":{"id":"orjhWUJgmxo5"},"source":["## Anexo: Mas detalles\n","\n","### Diferentes estrategias para combinar capas:"]},{"cell_type":"markdown","metadata":{"id":"P3D5qnRNmq5_"},"source":["\n","Los BERT autores de BERT exploraron diferentes estrategias para generar los embeddings de tokens:\n","\n","(Image tomada del blog de [Jay Allamar](http://jalammar.github.io/illustrated-bert/))\n","\n","\n","![alt text](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n","\n","Si bien la  concatenación de las últimas  4 capas produce los mejores resultados en esta evaluación, en realidad varios de los otros métodos dan resultados cercanos, y cual es mejor depende de cada caso.\n","\n","Esto ocurre porque las different capas de BERT codifican diferente tipo de información.\n"]},{"cell_type":"markdown","metadata":{"id":"jdw7cLJWMr_Y"},"source":["### Acerca de los tokens especiales:\n"]},{"cell_type":"markdown","metadata":{"id":"Jyx2kQxbnHbM"},"source":["Si bien en teoría el token `[CLS]` deberia actuar como una  an \"representacióin aggregada de los otros token\" para tareas de clasificacíon tasks, esto no es la mejor opción si lo que se quiere es sólo generar embeddings de frase. [De acuerdo a Jacob Devlin, uno de los autores de BERT](https://github.com/google-research/bert/issues/164): \"*I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations*.\"\n","\n","Sin embargo, el [CLS] token sí tiene efecto cuando se toma un modelo de embedding de BERT y se hace fine-tuning para problemas de clasificación. Ahí en la última capa, el valor de este token en la última capa se puede usar para clasificar.\n"]},{"cell_type":"markdown","metadata":{"id":"EbS8_z6XMuTJ"},"source":["\n","### Palabras fuera del vocabulario de entrenamiento\n","\n","Como las **palabras fuera del vocabulario de entrenamiento** pueden de todas maneras ser divididas (total o parcialmente) en varios tokens de n-gramas, podemos combinar esas partes para generar un embedding par la palabra.\n","\n","Las alternativas son:\n","* Promediar los embeddings (una técnica derivada de otros modelos con tokens de n-gramas como  fasttext), y\n","* Sumar los embeddings de lon n-gramas.\n"]},{"cell_type":"markdown","metadata":{"id":"OhbZxbKRxMvM"},"source":["## Origen de esta notebook\n","Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com\n"]}]}